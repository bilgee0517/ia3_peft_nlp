# PEFT IA3 Model for NLP

## Introduction

This project explores the implementation of Parameter Efficient Fine-Tuning (PEFT) in natural language processing using a customized DialoGPT model. The main goal is educational, focusing on learning about transformers and parameter-efficient tuning methods. An essential aspect of this project is addressing the high computational cost typically associated with fine-tuning large models. By using PEFT, we aim to make the fine-tuning process more feasible and cost-effective. The model is trained on a dataset from the TV series "Doctor Who," striving to develop a capable model for understanding and generating human-like text in dialogue systems, while being mindful of the limitations imposed by computational resources.

## Technologies Used
- Python
- PyTorch
- Transformers
- Pandas
- NumPy
- Datasets from Hugging Face
- Accelerate library

## Features

- Implementation of the DialoGPT model for dialogue generation.
- Exploration of IA3 (Injection of Adapters in Attention and Feedforward layers) for Parameter Efficient Fine-Tuning.
- Data preprocessing and customization for modeling dialogue from "Doctor Who."
- Detailed analysis of model architecture modifications and training process.


## Contact Information
[Linkedin](https://www.linkedin.com/in/bilegjargal-altangerel-6335ab25b/)|[Email](bilegjargal@uni.minerva.edu)| [Portfolio](https://obtainable-dart-e03.notion.site/Bilegjargal-Altangerel-Portfolio-f27a387c84d74f589e3cac8cce8d0d47?pvs=4) 